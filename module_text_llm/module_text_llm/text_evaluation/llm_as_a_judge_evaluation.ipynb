{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3826c961f4d8a45e",
   "metadata": {},
   "source": [
    "## 1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9daa2eefb7608",
   "metadata": {},
   "source": [
    "#### 1.1 Load data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.text_evaluation.data_processor import remove_feedback_titles\n",
    "\n",
    "remove_feedback_titles('data')"
   ],
   "id": "4cc2c0f97574adf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "558696778624a512",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_loader import load_data\n",
    "\n",
    "data = load_data('data')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f291f0ae2da91aa4",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_processor import process_data\n",
    "\n",
    "exercises = process_data(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fcf1a5944dfcc925",
   "metadata": {},
   "source": "#### 1.2 (Optional) Verify that data has expected format"
  },
  {
   "cell_type": "code",
   "id": "f30af1be83a617cb",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import display_exercise_summaries\n",
    "\n",
    "display_exercise_summaries(exercises, max_rows=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "491d01dc4b8c88fb",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import print_feedbacks\n",
    "\n",
    "print_feedbacks(exercises, exercise_id_to_find=544)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd74ecb349bcfa6b",
   "metadata": {},
   "source": [
    "## 2 Prepare LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7d07f05f05014",
   "metadata": {},
   "source": [
    "#### 2.1 Get model and adapt"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.text_evaluation.evaluation_schemas import Feedback, Submission\n",
    "from module_text_llm.prompts.llm_evaluation_prompt import system_message, human_message\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message)\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "def feedback_to_dict(feedback: Feedback, submission: Submission):\n",
    "    line_start, line_end = get_line_range_from_index_range(feedback.index_start, feedback.index_end, submission.text)\n",
    "    return {\n",
    "        \"description\": feedback.description,\n",
    "        \"line_start\": line_start,\n",
    "        \"line_end\": line_end\n",
    "    }"
   ],
   "id": "a9b2e7d685699c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dd58d166cabe9a",
   "metadata": {},
   "source": [
    "from module_text_llm.prompts.metrics import correctness, actionability, completeness, tone\n",
    "\n",
    "metrics = [actionability, correctness, completeness, tone]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f681bdb17464309",
   "metadata": {},
   "source": [
    "from module_text_llm.prompts.metrics import MetricEvaluations\n",
    "import json\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from module_text_llm.helpers.utils import add_sentence_numbers, get_line_range_from_index_range, format_grading_instructions\n",
    "from module_text_llm.text_evaluation.data_util import find_exercise_submission\n",
    "\n",
    "exercise, submission = find_exercise_submission(exercises, exercise_id_to_find = 4066)\n",
    "feedbacks = submission.feedbacks[\"CoFee\"]\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=MetricEvaluations)\n",
    "\n",
    "prompt_input = {\n",
    "    \"problem_statement\": exercise.problem_statement or \"No problem statement.\",\n",
    "    \"example_solution\": exercise.example_solution,\n",
    "    \"grading_instructions\": format_grading_instructions(exercise.grading_instructions, exercise.grading_criteria),\n",
    "    \"metrics\": json.dumps([metric.dict() for metric in metrics]),\n",
    "    \"format_instructions\": output_parser.get_format_instructions(),\n",
    "    \"submission\": add_sentence_numbers(submission.text),\n",
    "    \"feedbacks\": json.dumps([feedback_to_dict(feedback, submission) for feedback in feedbacks]),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(chat_prompt_template.format(**prompt_input))",
   "id": "ee73dd9d9db0b758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from module_text_llm.helpers.models import evaluation_model\n",
    "\n",
    "def get_logprobs_langchain(prompt_template: ChatPromptTemplate, prompt_input: dict, model: AzureChatOpenAI):\n",
    "    # Render the chat prompt template with the given inputs\n",
    "    prompt = prompt_template.format(**prompt_input)\n",
    "    \n",
    "    # Invoke the model with the formatted prompt\n",
    "    with get_openai_callback() as cb:\n",
    "        response = model.invoke(prompt, max_tokens=100, logprobs=True, top_logprobs=5, temperature=0)\n",
    "        print(f\"Total Cost (USD): ${format(cb.total_cost, '.6f')}\")\n",
    "        return response\n",
    "\n",
    "result = get_logprobs_langchain(chat_prompt_template, prompt_input, evaluation_model)"
   ],
   "id": "d6698e1f38f650e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import plot_top_logprobs\n",
    "\n",
    "# Parse the response using PydanticOutputParser\n",
    "parsed_response = output_parser.parse(result.content)\n",
    "\n",
    "# Assuming the result.response_metadata contains logprobs data for each evaluation\n",
    "logprobs_content = result.response_metadata['logprobs']['content']\n",
    "\n",
    "# Initialize a set to track used logprobs indices\n",
    "used_indices = set()\n",
    "\n",
    "# Iterate through each MetricEvaluation\n",
    "for i, evaluation in enumerate(parsed_response.evaluations):\n",
    "    score_str = str(evaluation.score)\n",
    "    \n",
    "    # Find the correct index for the score in the logprobs content\n",
    "    for j, logprobs_data in enumerate(logprobs_content):\n",
    "        if j not in used_indices and logprobs_data['token'] == score_str:\n",
    "            used_indices.add(j)  # Mark this index as used\n",
    "            logprobs = logprobs_data['top_logprobs']\n",
    "            selected_token = logprobs_data['token']\n",
    "            \n",
    "            # Plot the logprobs for the selected score\n",
    "            print(f\"Plotting logprobs for score '{score_str}' in metric '{evaluation.title}'\")\n",
    "            plot_top_logprobs(logprobs, selected_token)\n",
    "            break"
   ],
   "id": "336718a22795ed6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
