{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3826c961f4d8a45e",
   "metadata": {},
   "source": [
    "## 1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9daa2eefb7608",
   "metadata": {},
   "source": [
    "#### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "id": "558696778624a512",
   "metadata": {},
   "source": [
    "from pydantic.schema import datetime\n",
    "\n",
    "from module_text_llm.text_evaluation.data_loader import load_data\n",
    "\n",
    "data = load_data('data')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f291f0ae2da91aa4",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_processor import process_data\n",
    "\n",
    "exercises = process_data(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fcf1a5944dfcc925",
   "metadata": {},
   "source": "#### 1.2 (Optional) Verify that data has expected format"
  },
  {
   "cell_type": "code",
   "id": "f30af1be83a617cb",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import display_exercise_summaries\n",
    "\n",
    "display_exercise_summaries(exercises, max_rows=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "491d01dc4b8c88fb",
   "metadata": {},
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import print_feedbacks\n",
    "\n",
    "print_feedbacks(exercises, exercise_id_to_find=544)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd74ecb349bcfa6b",
   "metadata": {},
   "source": "## 2 Single LLM-as-a-Judge request"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.text_evaluation.prompts.llm_evaluation_prompt import get_formatted_prompt\n",
    "from module_text_llm.text_evaluation.prompts.metrics import correctness, actionability, completeness, tone\n",
    "from module_text_llm.text_evaluation.data_util import find_exercise_submission\n",
    "\n",
    "\n",
    "exercise, submission = find_exercise_submission(exercises, exercise_id_to_find=4066)\n",
    "metrics = [actionability, correctness, completeness, tone]\n",
    "\n",
    "assessments = {assessment.id: assessment for assessment in submission.assessments}\n",
    "assessment = assessments.get(\"LLM\")\n",
    "feedbacks = assessment.feedbacks\n",
    "\n",
    "\n",
    "prompt = get_formatted_prompt(exercise, submission, feedbacks, metrics)"
   ],
   "id": "570b56f011b1c03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.helpers.models import evaluation_model\n",
    "from module_text_llm.text_evaluation.llm_util import get_logprobs_langchain\n",
    "\n",
    "\n",
    "# TODO: Uncomment the following line to test the LLM model\n",
    "# assessment.meta['evaluation'] = get_logprobs_langchain(prompt, evaluation_model)"
   ],
   "id": "d6698e1f38f650e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot logprobs for the evaluation",
   "id": "318def40f86d06f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import plot_top_logprobs\n",
    "\n",
    "result = assessment.meta.get('evaluation')\n",
    "\n",
    "if not result is None:\n",
    "    # Parse the response using PydanticOutputParser\n",
    "    parsed_response = result.parsed_response\n",
    "    \n",
    "    # Assuming the result.response_metadata contains logprobs data for each evaluation\n",
    "    logprobs_content = result.response['response_metadata']['logprobs']['content']\n",
    "    \n",
    "    # Initialize a set to track used logprobs indices\n",
    "    used_indices = set()\n",
    "    \n",
    "    # Iterate through each MetricEvaluation\n",
    "    for i, evaluation in enumerate(parsed_response.evaluations):\n",
    "        score_str = str(evaluation.score)\n",
    "        \n",
    "        # Find the correct index for the score in the logprobs content\n",
    "        for j, logprobs_data in enumerate(logprobs_content):\n",
    "            if j not in used_indices and logprobs_data['token'] == score_str:\n",
    "                used_indices.add(j)  # Mark this index as used\n",
    "                logprobs = logprobs_data['top_logprobs']\n",
    "                selected_token = logprobs_data['token']\n",
    "                \n",
    "                # Plot the logprobs for the selected score\n",
    "                print(f\"Plotting logprobs for score '{score_str}' in metric '{evaluation.title}'\")\n",
    "                plot_top_logprobs(logprobs, selected_token)\n",
    "                break\n",
    "    "
   ],
   "id": "336718a22795ed6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Experiment with single vs multiple Metrics",
   "id": "11a6baa4a225092a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from module_text_llm.text_evaluation.prompts.llm_evaluation_prompt import get_formatted_prompt\n",
    "from module_text_llm.text_evaluation.prompts.metrics import correctness, actionability, completeness, tone\n",
    "\n",
    "# TODO: Uncomment the following lines to evaluate the submissions with multiple metrics\n",
    "# metrics = [actionability, correctness, completeness, tone]\n",
    "# print(\"# Prompt for all metrics:\")\n",
    "# for exercise in exercises:\n",
    "#     for submission in exercise.submissions:\n",
    "#         assessments = {assessment.id: assessment for assessment in submission.assessments}\n",
    "#         for assessmentStr in [\"Tutor\", \"LLM\", \"CoFee\"]:\n",
    "#             assessment = assessments.get(assessmentStr)\n",
    "#             feedbacks = assessment.feedbacks\n",
    "# \n",
    "#             print(f\"Prompt for {assessmentStr}, Exercise {exercise.id}, Submission {submission.id} with all metrics\")\n",
    "#             prompt = get_formatted_prompt(exercise, submission, feedbacks, metrics)\n",
    "#             result = get_logprobs_langchain(prompt, evaluation_model)\n",
    "#             assessment.meta['evaluation_all_metrics'] = result\n",
    "\n",
    "# TODO: Uncomment the following lines to evaluate the submissions with single metrics\n",
    "# print(\"# Prompt for single metrics:\")\n",
    "# for exercise in exercises:\n",
    "#     for submission in exercise.submissions:\n",
    "#         assessments = {assessment.id: assessment for assessment in submission.assessments}\n",
    "#         for assessmentStr in [\"Tutor\", \"LLM\", \"CoFee\"]:\n",
    "#             assessment = assessments.get(assessmentStr)\n",
    "#             feedbacks = assessment.feedbacks\n",
    "# \n",
    "#             for metric in metrics:\n",
    "#                 print(f\"Prompt for {assessmentStr}, Exercise {exercise.id}, Submission {submission.id} with single metric {metric.title}\")\n",
    "#                 prompt = get_formatted_prompt(exercise, submission, feedbacks, [metric])\n",
    "#                 result = get_logprobs_langchain(prompt, evaluation_model)\n",
    "#                 assessment.meta['evaluation_single_' + metric.title] = result"
   ],
   "id": "14bd6b61d6def73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Save the Exercises with Assessments and Evaluations to a JSON file",
   "id": "48b8f74857e8b9f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from module_text_llm.text_evaluation.evaluation_schemas import Exercise\n",
    "\n",
    "\n",
    "def save_to_json(data: Exercise, directory: str, filename: str) -> None:\n",
    "    \"\"\"Save the Exercise data to a JSON file in a specified directory.\"\"\"\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    json_data = json.loads(data.json())\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "directory = 'evaluations'\n",
    "filename = f'{exercise.id}_evaluation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "\n",
    "# Save the Exercise data to the specified directory with the generated filename\n",
    "save_to_json(exercise, directory, filename)"
   ],
   "id": "ab14a7a8a6dbf8b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load saved evaluations from JSON file",
   "id": "6c5a5049c328f75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "from module_text_llm.text_evaluation.evaluation_schemas import Exercise\n",
    "\n",
    "# TODO: Uncomment the following lines to restore the results from the JSON file\n",
    "directory = 'evaluations'\n",
    "filename = '4066_evaluation_20240830_071358.json'\n",
    "filepath = os.path.join(directory, filename)\n",
    "\n",
    "\n",
    "exercises = []\n",
    "with open(filepath, 'r') as file:\n",
    "    file_data = json.load(file)\n",
    "    exercise = Exercise.parse_obj(file_data)\n",
    "    exercises.append(exercise)\n",
    "\n",
    "print(f\"Loaded {len(exercises)} exercises from file '{filename}'\")\n",
    "    "
   ],
   "id": "8403687bc058dd83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Plot logprobs for saved evaluations",
   "id": "7657c7b1684566cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from module_text_llm.text_evaluation.data_visualization import plot_top_logprobs\n",
    "\n",
    "def plot_result(result):\n",
    "    if not result is None:\n",
    "        # Parse the response using PydanticOutputParser\n",
    "        parsed_response = result.get('parsed_response')\n",
    "        \n",
    "        # Assuming the result.response_metadata contains logprobs data for each evaluation\n",
    "        logprobs_content = result.get('response')['response_metadata']['logprobs']['content']\n",
    "        \n",
    "        # Initialize a set to track used logprobs indices\n",
    "        used_indices = set()\n",
    "        \n",
    "        # Iterate through each MetricEvaluation\n",
    "        for i, evaluation in enumerate(parsed_response.get('evaluations')):\n",
    "            score_str = str(evaluation.get('score'))\n",
    "            \n",
    "            # Find the correct index for the score in the logprobs content\n",
    "            for j, logprobs_data in enumerate(logprobs_content):\n",
    "                if j not in used_indices and logprobs_data['token'] == score_str:\n",
    "                    used_indices.add(j)  # Mark this index as used\n",
    "                    logprobs = logprobs_data['top_logprobs']\n",
    "                    selected_token = logprobs_data['token']\n",
    "                    \n",
    "                    # Plot the logprobs for the selected score\n",
    "                    print(f\"Plotting logprobs for score '{score_str}' in metric '{evaluation.get('title')}'\")\n",
    "                    plot_top_logprobs(logprobs, selected_token)\n",
    "                    break\n",
    "                    \n",
    "print(\"Plotting logprobs for all metrics\")\n",
    "for exercise in exercises:\n",
    "    for submission in exercise.submissions:\n",
    "        for assessment in submission.assessments:\n",
    "            result = assessment.meta.get('evaluation_all_metrics')\n",
    "            plot_result(result)\n",
    "            \n",
    "print(\"Plotting logprobs for single metrics\")\n",
    "for exercise in exercises:\n",
    "    for submission in exercise.submissions:\n",
    "        for assessment in submission.assessments:\n",
    "            for metric in [\"Actionability\", \"Correctness\", \"Completeness\", \"Tone\"]:\n",
    "                result = assessment.meta.get('evaluation_single_' + metric)\n",
    "                plot_result(result)\n",
    "            "
   ],
   "id": "9b2dc7fdfa7d2349"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
